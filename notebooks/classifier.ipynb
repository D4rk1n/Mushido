{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# Depending on library versions on your system, one of the following imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"/mnt/c/Users/Abdelrahman Tarek/Downloads/Dataset_Updated/data\"\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hsv_histogram(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    1. Resize the image to target_img_size using cv2.resize\n",
    "    2. Convert the image from BGR representation (cv2 is BGR not RGB) to HSV using cv2.cvtColor\n",
    "    3. Acquire the histogram using the cv2.calcHist. Apply the functions on the 3 channels. For the bins \n",
    "        parameter pass (8, 8, 8). For the ranges parameter pass ([0, 180, 0, 256, 0, 256]). Name the histogram\n",
    "        <hist>.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    hist = cv2.calcHist([img], [3], None, [8,8,8], [0, 180, 0, 256, 0, 256])\n",
    "    \n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "    return hist.flatten()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    You won't implement anything in this function. You just need to understand it \n",
    "    and understand its parameters (i.e win_size, cell_size, ... etc)\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    \n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_pixels(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    The classification algorithms we are going to use expect the input to be a vector not a matrix. \n",
    "    This is because they are general purpose and don't work only on images.\n",
    "    CNNs, on the other hand, expect matrices since they operate on images and exploit the \n",
    "    arrangement of pixels in the 2-D space.\n",
    "    \n",
    "    So, what we only need to do in this function is to resize and flatten the image.\n",
    "    \"\"\"\n",
    "    return cv2.resize(np.array(img).flatten(), target_img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, feature_set='hog'):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Given either 'hsv_hist', 'hog', 'raw', call the respective function and return its output\n",
    "    \"\"\"\n",
    "    if feature_set == 'hog':\n",
    "        return extract_hog_features(img)\n",
    "    elif feature_set == 'hsv_hist':\n",
    "        extract_hsv_histogram(img)\n",
    "    else:\n",
    "        extract_raw_pixels(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(feature_set='hog'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames = os.listdir(path_to_dataset)\n",
    "\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "        if fn.split('.')[-1] != 'png':\n",
    "            continue\n",
    "\n",
    "        label = fn.split('.')[0]\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_features(img, feature_set))\n",
    "        \n",
    "        # show an update every 1,000 images\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames)))\n",
    "        \n",
    "    return features, labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO understand the hyperparameters of each classifier\n",
    "classifiers = {\n",
    "    #'SVM': svm.LinearSVC(random_state=random_seed),\n",
    "    #'KNN': KNeighborsClassifier(n_neighbors=7),\n",
    "    'NN': MLPClassifier(solver='adam', random_state=random_seed, hidden_layer_sizes=(500,100,50), max_iter=50, verbose=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will test all our classifiers on a specific feature set\n",
    "def run_experiment(feature_set):\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    features, labels = load_dataset(feature_set)\n",
    "    print('Finished loading dataset.')\n",
    "    \n",
    "    # Since we don't want to know the performance of our classifier on images it has seen before\n",
    "    # we are going to withhold some images that we will test the classifier on after training \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    for model_name, model in classifiers.items():\n",
    "        print('############## Training', model_name, \"##############\")\n",
    "        # Train the model only on the training features\n",
    "        model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Test the model on images it hasn't seen before\n",
    "        accuracy = model.score(test_features, test_labels)\n",
    "        \n",
    "        print(model_name, 'accuracy:', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 1000/60817\n",
      "[INFO] processed 2000/60817\n",
      "[INFO] processed 3000/60817\n",
      "[INFO] processed 4000/60817\n",
      "[INFO] processed 5000/60817\n",
      "[INFO] processed 6000/60817\n",
      "[INFO] processed 7000/60817\n",
      "[INFO] processed 8000/60817\n",
      "[INFO] processed 9000/60817\n",
      "[INFO] processed 10000/60817\n",
      "[INFO] processed 11000/60817\n",
      "[INFO] processed 12000/60817\n",
      "[INFO] processed 13000/60817\n",
      "[INFO] processed 14000/60817\n",
      "[INFO] processed 15000/60817\n",
      "[INFO] processed 16000/60817\n",
      "[INFO] processed 17000/60817\n",
      "[INFO] processed 18000/60817\n",
      "[INFO] processed 19000/60817\n",
      "[INFO] processed 20000/60817\n",
      "[INFO] processed 21000/60817\n",
      "[INFO] processed 22000/60817\n",
      "[INFO] processed 23000/60817\n",
      "[INFO] processed 24000/60817\n",
      "[INFO] processed 25000/60817\n",
      "[INFO] processed 26000/60817\n",
      "[INFO] processed 27000/60817\n",
      "[INFO] processed 28000/60817\n",
      "[INFO] processed 29000/60817\n",
      "[INFO] processed 30000/60817\n",
      "[INFO] processed 31000/60817\n",
      "[INFO] processed 32000/60817\n",
      "[INFO] processed 33000/60817\n",
      "[INFO] processed 34000/60817\n",
      "[INFO] processed 35000/60817\n",
      "[INFO] processed 36000/60817\n",
      "[INFO] processed 37000/60817\n",
      "[INFO] processed 38000/60817\n",
      "[INFO] processed 39000/60817\n",
      "[INFO] processed 40000/60817\n",
      "[INFO] processed 41000/60817\n",
      "[INFO] processed 42000/60817\n",
      "[INFO] processed 43000/60817\n",
      "[INFO] processed 44000/60817\n",
      "[INFO] processed 45000/60817\n",
      "[INFO] processed 46000/60817\n",
      "[INFO] processed 47000/60817\n",
      "[INFO] processed 48000/60817\n",
      "[INFO] processed 49000/60817\n",
      "[INFO] processed 50000/60817\n",
      "[INFO] processed 51000/60817\n",
      "[INFO] processed 52000/60817\n",
      "[INFO] processed 53000/60817\n",
      "[INFO] processed 54000/60817\n",
      "[INFO] processed 55000/60817\n",
      "[INFO] processed 56000/60817\n",
      "[INFO] processed 57000/60817\n",
      "[INFO] processed 58000/60817\n",
      "[INFO] processed 59000/60817\n",
      "[INFO] processed 60000/60817\n",
      "Finished loading dataset.\n",
      "############## Training NN ##############\n",
      "Iteration 1, loss = 0.66841134\n",
      "Iteration 2, loss = 0.19834175\n",
      "Iteration 3, loss = 0.12563271\n",
      "Iteration 4, loss = 0.08532332\n",
      "Iteration 5, loss = 0.05977182\n",
      "Iteration 6, loss = 0.04299909\n",
      "Iteration 7, loss = 0.02983812\n",
      "Iteration 8, loss = 0.02115921\n",
      "Iteration 9, loss = 0.01505144\n",
      "Iteration 10, loss = 0.01984569\n",
      "Iteration 11, loss = 0.01138600\n",
      "Iteration 12, loss = 0.01740457\n",
      "Iteration 13, loss = 0.01129132\n",
      "Iteration 14, loss = 0.01454479\n",
      "Iteration 15, loss = 0.01779659\n",
      "Iteration 16, loss = 0.01274109\n",
      "Iteration 17, loss = 0.01150330\n",
      "Iteration 18, loss = 0.00626520\n",
      "Iteration 19, loss = 0.00213720\n",
      "Iteration 20, loss = 0.01481699\n",
      "Iteration 21, loss = 0.02093057\n",
      "Iteration 22, loss = 0.00959936\n",
      "Iteration 23, loss = 0.00676330\n",
      "Iteration 24, loss = 0.00769587\n",
      "Iteration 25, loss = 0.01033216\n",
      "Iteration 26, loss = 0.01431470\n",
      "Iteration 27, loss = 0.00652774\n",
      "Iteration 28, loss = 0.00825263\n",
      "Iteration 29, loss = 0.00282285\n",
      "Iteration 30, loss = 0.00352481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "NN accuracy: 96.11969746793818 %\n"
     ]
    }
   ],
   "source": [
    "run_experiment('hog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in classifiers.items():\n",
    "    filename = '/mnt/d/Projects/PYTHON/MusicSheetReader/music_notes_' + model_name + '.joblib.pkl'\n",
    "    _ = joblib.dump(model, filename, compress=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
